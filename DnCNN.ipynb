{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.models import *\n",
    "from keras.layers import Input,Conv2D,BatchNormalization,Activation,Lambda,Subtract\n",
    "import glob\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DnCNN():\n",
    "    \n",
    "    inpt = Input(shape=(None,None,1))\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(inpt)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    for i in range(15):\n",
    "        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
    "        x = BatchNormalization(axis=-1, epsilon=1e-3)(x)\n",
    "        x = Activation('relu')(x)   \n",
    "    \n",
    "    x = Conv2D(filters=1, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
    "    x = Subtract()([inpt, x])\n",
    "    model = Model(inputs=inpt, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img, mode=0):\n",
    "    if mode == 0:\n",
    "        return img\n",
    "    elif mode == 1:\n",
    "        return np.flipud(img)\n",
    "    elif mode == 2:\n",
    "        return np.rot90(img)\n",
    "    elif mode == 3:\n",
    "        return np.flipud(np.rot90(img))\n",
    "    elif mode == 4:\n",
    "        return np.rot90(img, k=2)\n",
    "    elif mode == 5:\n",
    "        return np.flipud(np.rot90(img, k=2))\n",
    "    elif mode == 6:\n",
    "        return np.rot90(img, k=3)\n",
    "    elif mode == 7:\n",
    "        return np.flipud(np.rot90(img, k=3))\n",
    "    \n",
    "def gen_patches(file_name):\n",
    "\n",
    "    img = cv2.imread(file_name, 0) \n",
    "    h, w = img.shape\n",
    "    scales = [1, 0.9, 0.8, 0.7]\n",
    "    patches = []\n",
    "\n",
    "    for scale in scales:\n",
    "        h_scaled, w_scaled = int(h*scale),int(w*scale)\n",
    "        img_scaled = cv2.resize(img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        for i in range(0, h_scaled-patch_size+1, stride):\n",
    "            for j in range(0, w_scaled-patch_size+1, stride):\n",
    "                x = img_scaled[i:i+patch_size, j:j+patch_size]\n",
    "                \n",
    "                for k in range(0, aug_times):\n",
    "                    x_aug = data_aug(x, mode=np.random.randint(0,8))\n",
    "                    patches.append(x_aug)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size, stride = 20, 20\n",
    "aug_times = 1\n",
    "src_dir = './data/Train400/'\n",
    "save_dir = './data/npy_data/'\n",
    "file_list = glob.glob(src_dir+'*.png') \n",
    "num_threads = 16    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(0,len(file_list),num_threads):\n",
    "    \n",
    "    p = Pool(num_threads)\n",
    "    patch = p.map(gen_patches,file_list[i:min(i+num_threads,len(file_list))])\n",
    "    \n",
    "    for x in patch:\n",
    "        res += x\n",
    "\n",
    "    print('Picture '+str(i)+' to '+str(i+num_threads)+' are finished.')\n",
    "\n",
    "res = np.array(res, dtype='uint8')\n",
    "print('Shape of result: ' + str(res.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models='DnCNN'\n",
    "name = 'dncnn' \n",
    "batch_size=1024\n",
    "train_data= res\n",
    "test_dir='./data/Test/'\n",
    "sigma=25\n",
    "epoch=10\n",
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch%10 == 0:\n",
    "        lr = lr/2\n",
    "    \n",
    "    return lr\n",
    "\n",
    "\n",
    "def train_generator(data, batch_size=8):\n",
    "    \n",
    "    indices = list(range(data.shape[0]))\n",
    "    while(True):\n",
    "        np.random.shuffle(indices)   \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch = data[indices[i:i+batch_size]]\n",
    "            noise =  np.random.normal(0, sigma/255.0, batch.shape) \n",
    "            transformed_batch = batch + noise\n",
    "            yield transformed_batch, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "np.random.shuffle(data)\n",
    "valid_y = data[int(n*0.85):]\n",
    "train_data = data[:int(n*0.85)]\n",
    "\n",
    "valid_x = valid_y.copy()\n",
    "for img in range(len(valid_x)):\n",
    "    valid_x[img] = valid_x[img] + np.random.normal(0, sigma/255.0, valid_x[img].shape)\n",
    "    valid_x[img] = valid_x[img].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss=['mse'])\n",
    "\n",
    "hist = model.fit_generator(train_generator(train_data, batch_size=batch_size),\n",
    "            steps_per_epoch=len(data)//batch_size,\n",
    "            validation_data = (valid_x, valid_y),\n",
    "            epochs=epoch, \n",
    "            verbose=1, \n",
    "            callbacks=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_values = hist.history['loss']\n",
    "val_loss_values = hist.history['val_loss']\n",
    "\n",
    "epochs = hist.epochs\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('{}/*.png'.format(test_dir))\n",
    "\n",
    "for file in file_list:\n",
    "    img_clean = np.array(Image.open(file), dtype='uint8')\n",
    "    img_clean_scaled = np.array(img_clean, dtype='float32') / 255.0\n",
    "    img_test = img_clean_scaled + np.random.normal(0, sigma/255.0, img_clean.shape)\n",
    "    img_test = img_test.astype('float32')\n",
    "\n",
    "    x_test = img_test.reshape(1, img_test.shape[0], img_test.shape[1], 1) \n",
    "    y_predict = model.predict(x_test)\n",
    "\n",
    "    img_out = y_predict.reshape(img_clean.shape)\n",
    "\n",
    "\n",
    "    img_out = np.clip(img_out, 0, 1)\n",
    "    img_out = Image.fromarray((img_out*255).astype('uint8')) \n",
    "\n",
    "    print('MSE test: ', mean_squared_error(img_out, img_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "\n",
    "with open(name+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(name+\".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
